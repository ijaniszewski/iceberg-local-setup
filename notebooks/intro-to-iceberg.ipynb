{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cabc966-dc04-4d1b-92af-b4de1324dab4",
   "metadata": {},
   "source": [
    "### MinIO explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6925cf3-92dc-4185-9ee7-53fc56bed26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '184BFD72F6D07E16',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '0',\n",
       "   'etag': '\"3af42309382afb590d9143564b4bb8b8\"',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-checksum-crc32': 'fW13PQ==',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '184BFD72F6D07E16',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '1036',\n",
       "   'x-ratelimit-remaining': '1036',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Tue, 24 Jun 2025 13:25:50 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"3af42309382afb590d9143564b4bb8b8\"',\n",
       " 'ChecksumCRC32': 'fW13PQ=='}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# These come from your docker-compose env vars\n",
    "aws_access_key_id = os.environ[\"AWS_ACCESS_KEY_ID\"]     # \"admin\"\n",
    "aws_secret_access_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]  # \"password\"\n",
    "aws_region = os.environ[\"AWS_REGION\"] # us-east-1\n",
    "\n",
    "# Mocked S3 client that connects to local MinIO\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio:9000\",  # Local MinIO service\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_region\n",
    ")\n",
    "\n",
    "# Create a bucket\n",
    "s3.create_bucket(Bucket=\"poc\")\n",
    "\n",
    "# Create a blob (upload a file)\n",
    "s3.put_object(Bucket=\"poc\", Key=\"demo.txt\", Body=b\"Hello, Iceberg!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5fb31f-d2ae-4f1a-b2ab-06611db6f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found object: demo.txt\n"
     ]
    }
   ],
   "source": [
    "# List all the blobs\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=\"poc\")\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    print(f\"Found object: {obj['Key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062b77f6-284d-4710-b1f1-051945aec80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Iceberg!\n"
     ]
    }
   ],
   "source": [
    "# Read the blob\n",
    "\n",
    "response = s3.get_object(Bucket=\"poc\", Key=\"demo.txt\")\n",
    "print(response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03613e3a-8759-4993-afaf-debb3268a1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '184BFD7312AA1DF2',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '184BFD7312AA1DF2',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '1036',\n",
       "   'x-ratelimit-remaining': '1036',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Tue, 24 Jun 2025 13:25:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean it up\n",
    "\n",
    "# Delete blob\n",
    "s3.delete_object(Bucket=\"poc\", Key=\"demo.txt\")\n",
    "\n",
    "# Delete bucket\n",
    "s3.delete_bucket(Bucket=\"poc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21232b4-f564-41e1-8c0a-07b977a09071",
   "metadata": {},
   "source": [
    "### Iceberg time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8846fe4-3520-495b-b03f-1a754684bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyiceberg.catalog import load_rest\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError, TableAlreadyExistsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1371524-5cd3-4000-b685-ba596a03cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyiceberg.catalog import load_rest\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError, TableAlreadyExistsError\n",
    "\n",
    "catalog = load_rest(\n",
    "    name=\"rest\",\n",
    "    conf = {\n",
    "        \"uri\": \"http://rest:8181/\",\n",
    "        \"s3.endpoint\": \"http://minio:9000\",\n",
    "        \"s3.access-key\": aws_access_key_id,\n",
    "        \"s3.secret-key\": aws_secret_access_key\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f406b19a-ba89-4dbe-bda1-dd3d83a428e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError\n",
    "\n",
    "namespace = \"rideshare\"\n",
    "\n",
    "try:\n",
    "    catalog.create_namespace(namespace)\n",
    "except NamespaceAlreadyExistsError:\n",
    "    pass  # It's fine if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deec08c7-d43f-490c-9837-de73f497eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces: [('rideshare',)]\n"
     ]
    }
   ],
   "source": [
    "namespaces = catalog.list_namespaces()\n",
    "print(\"Namespaces:\", namespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "defddb68-d600-48f9-9da0-2c66f59b7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, imports\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    NestedField, UUIDType, StringType, TimestampType, DoubleType\n",
    ")\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2268b6f9-b475-4bb5-9ce2-c8bf370992d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"ride_id\", field_type=UUIDType(), required=True),\n",
    "    NestedField(field_id=2, name=\"driver_id\", field_type=StringType()),\n",
    "    NestedField(field_id=3, name=\"customer_id\", field_type=StringType()),\n",
    "    NestedField(field_id=4, name=\"pickup_time\", field_type=TimestampType()),\n",
    "    NestedField(field_id=5, name=\"dropoff_time\", field_type=TimestampType()),\n",
    "    NestedField(field_id=6, name=\"fare\", field_type=DoubleType()),\n",
    "    NestedField(field_id=7, name=\"pickup_location\", field_type=StringType()),\n",
    "    NestedField(field_id=8, name=\"dropoff_location\", field_type=StringType()),\n",
    "    NestedField(field_id=9, name=\"status\", field_type=StringType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a723efd3-9f8d-4aaa-9e85-4a75a385366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_partition_spec = PartitionSpec(\n",
    "    fields=[\n",
    "        PartitionField(\n",
    "            source_id=4,       # pickup_time\n",
    "            field_id=1000,     # unique ID for this partition\n",
    "            transform=\"day\",   # group by day\n",
    "            name=\"pickup_day\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d3ba4da-eef6-483f-9fdb-330a0e7e3b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rides(\n",
       "  1: ride_id: required uuid,\n",
       "  2: driver_id: optional string,\n",
       "  3: customer_id: optional string,\n",
       "  4: pickup_time: optional timestamp,\n",
       "  5: dropoff_time: optional timestamp,\n",
       "  6: fare: optional double,\n",
       "  7: pickup_location: optional string,\n",
       "  8: dropoff_location: optional string,\n",
       "  9: status: optional string\n",
       "),\n",
       "partition by: [pickup_day],\n",
       "sort order: [],\n",
       "snapshot: null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.create_table(\n",
    "    identifier=f\"{namespace}.rides\",\n",
    "    schema=rides_schema,\n",
    "    partition_spec=rides_partition_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce808d95-77d6-4267-b64c-0e150130cdb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: rideshare/rides/metadata/00000-f41ba4cc-4686-4c8b-98c1-290e8742bcdc.metadata.json\n",
      "\n",
      "--- Parsed metadata.json ---\n",
      "\n",
      "{\n",
      "  \"format-version\": 2,\n",
      "  \"table-uuid\": \"4fce4b55-e6b4-4ea0-9597-3603e62e9fec\",\n",
      "  \"location\": \"s3://warehouse/rideshare/rides\",\n",
      "  \"last-sequence-number\": 0,\n",
      "  \"last-updated-ms\": 1750771559065,\n",
      "  \"last-column-id\": 9,\n",
      "  \"current-schema-id\": 0,\n",
      "  \"schemas\": [\n",
      "    {\n",
      "      \"type\": \"struct\",\n",
      "      \"schema-id\": 0,\n",
      "      \"fields\": [\n",
      "        {\n",
      "          \"id\": 1,\n",
      "          \"name\": \"ride_id\",\n",
      "          \"required\": true,\n",
      "          \"type\": \"uuid\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 2,\n",
      "          \"name\": \"driver_id\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 3,\n",
      "          \"name\": \"customer_id\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 4,\n",
      "          \"name\": \"pickup_time\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"timestamp\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 5,\n",
      "          \"name\": \"dropoff_time\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"timestamp\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 6,\n",
      "          \"name\": \"fare\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"double\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 7,\n",
      "          \"name\": \"pickup_location\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 8,\n",
      "          \"name\": \"dropoff_location\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "          \"id\": 9,\n",
      "          \"name\": \"status\",\n",
      "          \"required\": false,\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"default-spec-id\": 0,\n",
      "  \"partition-specs\": [\n",
      "    {\n",
      "      \"spec-id\": 0,\n",
      "      \"fields\": [\n",
      "        {\n",
      "          \"name\": \"pickup_day\",\n",
      "          \"transform\": \"day\",\n",
      "          \"source-id\": 4,\n",
      "          \"field-id\": 1000\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"last-partition-id\": 1000,\n",
      "  \"default-sort-order-id\": 0,\n",
      "  \"sort-orders\": [\n",
      "    {\n",
      "      \"order-id\": 0,\n",
      "      \"fields\": []\n",
      "    }\n",
      "  ],\n",
      "  \"properties\": {\n",
      "    \"write.parquet.compression-codec\": \"zstd\"\n",
      "  },\n",
      "  \"current-snapshot-id\": -1,\n",
      "  \"refs\": {},\n",
      "  \"snapshots\": [],\n",
      "  \"statistics\": [],\n",
      "  \"partition-statistics\": [],\n",
      "  \"snapshot-log\": [],\n",
      "  \"metadata-log\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=\"warehouse\", Prefix=\"rideshare/rides/metadata/\")\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    obj_name = obj[\"Key\"]\n",
    "    print(f\"Found file: {obj_name}\")\n",
    "    \n",
    "    if obj_name.endswith(\"metadata.json\"):\n",
    "        response = s3.get_object(Bucket=\"warehouse\", Key=obj_name)\n",
    "        content = response[\"Body\"].read().decode()\n",
    "        metadata = json.loads(content)\n",
    "        \n",
    "        print(\"\\n--- Parsed metadata.json ---\\n\")\n",
    "        print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c558f-fe68-4ba7-8837-a986234c6720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522f8c38-1a98-444b-8384-1150444d179e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docker'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     f.write(sql)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocker\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-i\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflink_sql_client\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/opt/flink/bin/sql-client.sh\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/opt/flink/sql/insert.sql\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     31\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:548\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    545\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m    546\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    550\u001b[39m         stdout, stderr = process.communicate(\u001b[38;5;28minput\u001b[39m, timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1022\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:1955\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[39m\n\u001b[32m   1953\u001b[39m     err_msg = os.strerror(errno_num)\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[32m   1956\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1957\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'docker'"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "CREATE CATALOG iceberg_catalog WITH (\n",
    "  'type'='iceberg',\n",
    "  'catalog-type'='rest',\n",
    "  'uri'='http://rest:8181',\n",
    "  'warehouse'='s3://warehouse',\n",
    "  'io-impl'='org.apache.iceberg.aws.s3.S3FileIO',\n",
    "  's3.endpoint'='http://minio:9000',\n",
    "  's3.path-style-access'='true',\n",
    "  'aws.region'='us-east-1',\n",
    "  'aws.access-key-id'='admin',\n",
    "  'aws.secret-access-key'='password'\n",
    ");\n",
    "\n",
    "USE CATALOG iceberg_catalog;\n",
    "USE rideshare;\n",
    "\n",
    "INSERT INTO rides VALUES\n",
    "  ('r1', TIMESTAMP '2025-06-01 08:15:00', 19.80),\n",
    "  ('r2', TIMESTAMP '2025-06-02 14:30:00', 13.50);\n",
    "\"\"\"\n",
    "\n",
    "with open(\"/sql/insert.sql\", \"w\") as f:\n",
    "    f.write(sql)\n",
    "\n",
    "import subprocess\n",
    "subprocess.run([\n",
    "    \"docker\", \"exec\", \"-i\", \"flink_sql_client\",\n",
    "    \"/opt/flink/bin/sql-client.sh\",\n",
    "    \"-f\", \"/opt/flink/sql/insert.sql\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf4b78d-2201-4ea5-84cb-2fb0713c8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f961d52d-8f61-4f71-ac46-f5ffab94cdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jun 24, 2025 1:26:54 PM org.jline.utils.Log logr\n",
      "WARNING: Unable to create a system terminal, creating a dumb terminal (enable debug logging for more information)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34;1m[INFO] Executing SQL from file.\u001b[0m\n",
      "\n",
      "Command history file path: /root/.flink-sql-history\n",
      "Flink SQL> CREATE CATALOG iceberg_catalog WITH (\n",
      ">   'type'='iceberg',\n",
      ">   'catalog-type'='rest',\n",
      ">   'uri'='http://rest:8181',\n",
      ">   'warehouse'='s3://warehouse',\n",
      ">   'io-impl'='org.apache.iceberg.aws.s3.S3FileIO',\n",
      ">   's3.endpoint'='http://minio:9000',\n",
      ">   's3.path-style-access'='true',\n",
      ">   'aws.region'='us-east-1',\n",
      ">   'aws.access-key-id'='admin',\n",
      ">   'aws.secret-access-key'='password'\n",
      "> )\u001b[31;1m[ERROR] Could not execute SQL statement. Reason:\n",
      "org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'iceberg' that implements 'org.apache.flink.table.factories.CatalogFactory' in the classpath.\n",
      "\n",
      "Available factory identifiers are:\n",
      "\n",
      "generic_in_memory\u001b[0m\n",
      "\n",
      "Shutting down the session...\n",
      "done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/opt/flink/bin/sql-client.sh', '-f', '/sql/insert.sql'], returncode=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\n",
    "    \"/opt/flink/bin/sql-client.sh\",\n",
    "    \"-f\", \"/sql/insert.sql\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2ac0c-43b9-4651-97dc-40afade31ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23abfbc-1f02-4c51-a8da-cd5807bb81b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3db3b5-43e6-4596-892f-a926a4670a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc77d3-5a69-4ef2-8deb-92af44aeb81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984f7a8-a594-4a79-9c1c-609ec040d5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ba18f-c56c-4c63-8f7c-4499eb41c6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be3259b8-7c83-4f6c-84c8-f5afac8c2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "print(duckdb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "51c3fafe-59d9-4d5f-9f33-71f8ae44179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rideshare.rides\n"
     ]
    }
   ],
   "source": [
    "tables = catalog.list_tables(\"rideshare\")\n",
    "for namespace, table_name in catalog.list_tables(\"rideshare\"):\n",
    "    print(f\"{namespace}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "06afb757-098e-477d-9f01-8aa94651d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://warehouse/rideshare/rides\n"
     ]
    }
   ],
   "source": [
    "table = catalog.load_table((\"rideshare\", \"rides\"))  # or \"rideshare.rides\"\n",
    "print(table.location())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "344c2922-c79f-4271-8850-27645a2811bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcadd4ca8b0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.execute(\"INSTALL iceberg;\")\n",
    "con.execute(\"LOAD iceberg;\")\n",
    "con.execute(\"UPDATE EXTENSIONS;\")\n",
    "\n",
    "con.execute(\"SET s3_region TO 'us-east-1'\")\n",
    "con.execute(\"SET s3_access_key_id TO 'admin'\")\n",
    "con.execute(\"SET s3_secret_access_key TO 'password'\")\n",
    "con.execute(\"SET s3_endpoint TO 'minio:9000'\")\n",
    "con.execute(\"SET s3_use_ssl TO false\")  # ✅ THIS is the key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1d0d4056-2de1-4ae3-9a80-50dc4daf0939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcadd4ca8b0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if DuckDB can write to storage\n",
    "df = pd.DataFrame([{\"x\": 1, \"y\": \"test\"}])\n",
    "con.register(\"mydf\", df)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    COPY mydf TO 's3://warehouse/test-parquet/test.parquet' (FORMAT 'parquet');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ccf9906e-a4fa-4d6c-922a-e8971f959492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"ride_id\": str(uuid.uuid4()),\n",
    "        \"driver_id\": \"d1\",\n",
    "        \"customer_id\": \"c1\",\n",
    "        \"pickup_time\": datetime(2024, 6, 1, 8, 15),\n",
    "        \"dropoff_time\": datetime(2024, 6, 1, 8, 45),\n",
    "        \"fare\": 19.80,\n",
    "        \"pickup_location\": \"Downtown\",\n",
    "        \"dropoff_location\": \"Airport\",\n",
    "        \"status\": \"completed\"\n",
    "    },\n",
    "    {\n",
    "        \"ride_id\": str(uuid.uuid4()),\n",
    "        \"driver_id\": \"d2\",\n",
    "        \"customer_id\": \"c2\",\n",
    "        \"pickup_time\": datetime(2024, 6, 2, 14, 30),\n",
    "        \"dropoff_time\": datetime(2024, 6, 2, 14, 55),\n",
    "        \"fare\": 13.50,\n",
    "        \"pickup_location\": \"Midtown\",\n",
    "        \"dropoff_location\": \"Suburbs\",\n",
    "        \"status\": \"completed\"\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2d2b7189-d2f7-484a-b64f-975d49ea255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcadd4ca8b0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.register(\"rides_df\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "198f86c6-c5df-4bc7-bc17-da17b98f85db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcadd4ca8b0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "    CREATE TABLE 's3://warehouse/rideshare/rides' AS\n",
    "    SELECT * FROM rides_df\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ce5ed669-9e79-4393-b0ab-e1a409f83e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>pickup_time</th>\n",
       "      <th>dropoff_time</th>\n",
       "      <th>fare</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_location</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39945a6b-9cd1-4389-9e93-5e55f39fbf0d</td>\n",
       "      <td>d1</td>\n",
       "      <td>c1</td>\n",
       "      <td>2024-06-01 08:15:00</td>\n",
       "      <td>2024-06-01 08:45:00</td>\n",
       "      <td>19.8</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>Airport</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8c164803-8e00-485e-95d4-4cfd81344fc8</td>\n",
       "      <td>d2</td>\n",
       "      <td>c2</td>\n",
       "      <td>2024-06-02 14:30:00</td>\n",
       "      <td>2024-06-02 14:55:00</td>\n",
       "      <td>13.5</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>Suburbs</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ride_id driver_id customer_id  \\\n",
       "0  39945a6b-9cd1-4389-9e93-5e55f39fbf0d        d1          c1   \n",
       "1  8c164803-8e00-485e-95d4-4cfd81344fc8        d2          c2   \n",
       "\n",
       "          pickup_time        dropoff_time  fare pickup_location  \\\n",
       "0 2024-06-01 08:15:00 2024-06-01 08:45:00  19.8        Downtown   \n",
       "1 2024-06-02 14:30:00 2024-06-02 14:55:00  13.5         Midtown   \n",
       "\n",
       "  dropoff_location     status  \n",
       "0          Airport  completed  \n",
       "1          Suburbs  completed  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM 's3://warehouse/rideshare/rides'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "add4904a-2b7f-45d2-9c8a-a85d3f95ecc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Unhandled options found: catalog_type, storage_namespace",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      2\u001b[39m \u001b[33;43mATTACH \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://warehouse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AS warehouse\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[33;43m(\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[33;43m    TYPE ICEBERG,\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43m    CATALOG_TYPE HADOOP,\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m    STORAGE_NAMESPACE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://warehouse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m    AUTHORIZATION_TYPE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m)\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mError\u001b[39m: Unhandled options found: catalog_type, storage_namespace"
     ]
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "ATTACH 's3://warehouse' AS warehouse\n",
    "(\n",
    "    TYPE ICEBERG,\n",
    "    CATALOG_TYPE HADOOP,\n",
    "    STORAGE_NAMESPACE 's3://warehouse',\n",
    "    AUTHORIZATION_TYPE 'none'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "307acfa1-80d9-4e65-bab6-855c4161eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcadde9e830>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert more sample rides\n",
    "df2 = pd.DataFrame([\n",
    "    {\n",
    "        \"ride_id\": str(uuid.uuid4()),\n",
    "        \"driver_id\": \"d3\",\n",
    "        \"customer_id\": \"c3\",\n",
    "        \"pickup_time\": datetime(2024, 6, 3, 9, 15),\n",
    "        \"dropoff_time\": datetime(2024, 6, 3, 9, 45),\n",
    "        \"fare\": 29.99,\n",
    "        \"pickup_location\": \"OldTown\",\n",
    "        \"dropoff_location\": \"CityCenter\",\n",
    "        \"status\": \"completed\"\n",
    "    }\n",
    "])\n",
    "\n",
    "con.register(\"rides_df2\", df2)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    INSERT INTO 's3://warehouse/rideshare/rides'\n",
    "    SELECT * FROM rides_df2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6ce52c6-fdcb-48bf-9252-74eea67ed67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>pickup_time</th>\n",
       "      <th>dropoff_time</th>\n",
       "      <th>fare</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_location</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bf986a84-6769-4008-aa9b-cc74c5df4297</td>\n",
       "      <td>d1</td>\n",
       "      <td>c1</td>\n",
       "      <td>2024-06-01 08:15:00</td>\n",
       "      <td>2024-06-01 08:45:00</td>\n",
       "      <td>19.80</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>Airport</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ac2471f-6cec-41cc-a1c2-86435c01cda2</td>\n",
       "      <td>d2</td>\n",
       "      <td>c2</td>\n",
       "      <td>2024-06-02 14:30:00</td>\n",
       "      <td>2024-06-02 14:55:00</td>\n",
       "      <td>13.50</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>Suburbs</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d82f8fa-6198-4358-854b-2cc5ace076b6</td>\n",
       "      <td>d3</td>\n",
       "      <td>c3</td>\n",
       "      <td>2024-06-03 09:15:00</td>\n",
       "      <td>2024-06-03 09:45:00</td>\n",
       "      <td>29.99</td>\n",
       "      <td>OldTown</td>\n",
       "      <td>CityCenter</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ride_id driver_id customer_id  \\\n",
       "0  bf986a84-6769-4008-aa9b-cc74c5df4297        d1          c1   \n",
       "1  5ac2471f-6cec-41cc-a1c2-86435c01cda2        d2          c2   \n",
       "2  7d82f8fa-6198-4358-854b-2cc5ace076b6        d3          c3   \n",
       "\n",
       "          pickup_time        dropoff_time   fare pickup_location  \\\n",
       "0 2024-06-01 08:15:00 2024-06-01 08:45:00  19.80        Downtown   \n",
       "1 2024-06-02 14:30:00 2024-06-02 14:55:00  13.50         Midtown   \n",
       "2 2024-06-03 09:15:00 2024-06-03 09:45:00  29.99         OldTown   \n",
       "\n",
       "  dropoff_location     status  \n",
       "0          Airport  completed  \n",
       "1          Suburbs  completed  \n",
       "2       CityCenter  completed  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM 's3://warehouse/rideshare/rides'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678185e-950e-4259-9dca-8a8b0a160d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ab1c5c37-4f84-41f5-84d9-9511b6c5c8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcad823bdf0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute('DROP TABLE \"s3://warehouse/rideshare/rides\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "464993bb-f833-4c1b-91e2-82e81690cdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcadd539230>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "    CREATE TABLE 's3://warehouse/rideshare/rides' AS\n",
    "    SELECT * FROM rides_df2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6fbcd87-abee-4143-999b-c365709db588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>pickup_time</th>\n",
       "      <th>dropoff_time</th>\n",
       "      <th>fare</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_location</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5c3136f2-8f59-48b4-9b01-0055348a92e8</td>\n",
       "      <td>d3</td>\n",
       "      <td>c3</td>\n",
       "      <td>2024-06-03 09:15:00</td>\n",
       "      <td>2024-06-03 09:45:00</td>\n",
       "      <td>29.99</td>\n",
       "      <td>OldTown</td>\n",
       "      <td>CityCenter</td>\n",
       "      <td>completed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ride_id driver_id customer_id  \\\n",
       "0  5c3136f2-8f59-48b4-9b01-0055348a92e8        d3          c3   \n",
       "\n",
       "          pickup_time        dropoff_time   fare pickup_location  \\\n",
       "0 2024-06-03 09:15:00 2024-06-03 09:45:00  29.99         OldTown   \n",
       "\n",
       "  dropoff_location     status  \n",
       "0       CityCenter  completed  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM 's3://warehouse/rideshare/rides'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c68f1013-5452-4c72-87d8-fab394bd5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(Bucket=\"warehouse\", Prefix=\"rideshare/rides/data/\")\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"], obj[\"Size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63e669d3-8dc6-4e5d-87e0-8edc9b249c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcadc118770>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute('DROP TABLE IF EXISTS \"s3://warehouse/rideshare/rides\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6353e686-76a7-458a-b9a9-ad0562242565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x     y\n",
       "0  1  test"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d71c65a-7b08-4afe-b552-77492d880010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcad823bdf0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "con.execute(\"SET s3_region TO 'us-east-1'\")\n",
    "con.execute(\"SET s3_access_key_id TO 'admin'\")\n",
    "con.execute(\"SET s3_secret_access_key TO 'password'\")\n",
    "con.execute(\"SET s3_endpoint TO 'minio:9000'\")\n",
    "con.execute(\"SET s3_url_style TO 'path'\")\n",
    "con.execute(\"SET s3_use_ssl TO false\")  # ✅ THIS is the key!\n",
    "\n",
    "con.register(\"rides_df\", df)\n",
    "\n",
    "# ✅ Use the raw S3 path to the Iceberg table\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE 's3://warehouse/rideshare/rides' AS\n",
    "    SELECT * FROM rides_df\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a0d433d-1382-4832-bebd-06e0ad91c70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcad823bdf0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([{\"x\": 1, \"y\": \"test\"}])\n",
    "con.register(\"mydf\", df)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    COPY mydf TO 's3://warehouse/test-parquet/test.parquet' (FORMAT 'parquet');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2ef42eb-65f0-4b10-acb5-d41a836df6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x     y\n",
       "0  1  test"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM 's3://warehouse/rideshare/rides'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a5e91963-376e-4cb0-bf4a-35d458e0ba6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fcad823bdf0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([{\"x\": 1, \"y\": \"test\"}])\n",
    "con.register(\"mydf\", df)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    COPY mydf TO 's3://warehouse/test-parquet/test.parquet' (FORMAT 'parquet');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5c57c691-84c1-43f2-877f-9ab259ee9f62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BucketAlreadyOwnedByYou",
     "evalue": "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBucketAlreadyOwnedByYou\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ms3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwarehouse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/botocore/client.py:595\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    592\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    593\u001b[39m     )\n\u001b[32m    594\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/botocore/context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/botocore/client.py:1058\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1054\u001b[39m     error_code = error_info.get(\u001b[33m\"\u001b[39m\u001b[33mQueryErrorCode\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info.get(\n\u001b[32m   1055\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1056\u001b[39m     )\n\u001b[32m   1057\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[31mBucketAlreadyOwnedByYou\u001b[39m: An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it."
     ]
    }
   ],
   "source": [
    "s3.create_bucket(Bucket=\"warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c08f2559-e374-400f-a253-9b711fe72260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '184BEFCA3E0C4B10',\n",
       "  'HostId': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'accept-ranges': 'bytes',\n",
       "   'content-length': '367',\n",
       "   'content-type': 'application/xml',\n",
       "   'server': 'MinIO',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'vary': 'Origin, Accept-Encoding',\n",
       "   'x-amz-id-2': 'dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8',\n",
       "   'x-amz-request-id': '184BEFCA3E0C4B10',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'x-ratelimit-limit': '1588',\n",
       "   'x-ratelimit-remaining': '1588',\n",
       "   'x-xss-protection': '1; mode=block',\n",
       "   'date': 'Tue, 24 Jun 2025 09:15:32 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'warehouse',\n",
       "   'CreationDate': datetime.datetime(2025, 6, 23, 14, 34, 21, 19000, tzinfo=tzlocal())}],\n",
       " 'Owner': {'DisplayName': 'minio',\n",
       "  'ID': '02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4'}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68bacd91-c931-43ae-9817-80b2751d6a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: Cannot open file \"s3://warehouse/test-output/rides.parquet\": No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIOException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m con.register(\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m, df)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Try writing directly to S3 (just as Parquet)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[33;43m    COPY df TO \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://warehouse/test-output/rides.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[33;43m    (FORMAT \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m);\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIOException\u001b[39m: IO Error: Cannot open file \"s3://warehouse/test-output/rides.parquet\": No such file or directory"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample row\n",
    "df = pd.DataFrame([{\n",
    "    \"ride_id\": str(uuid.uuid4()),\n",
    "    \"driver_id\": \"d100\",\n",
    "    \"pickup_time\": datetime.now(),\n",
    "    \"fare\": 99.99\n",
    "}])\n",
    "\n",
    "# Connect\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Register\n",
    "con.register(\"df\", df)\n",
    "\n",
    "# Try writing directly to S3 (just as Parquet)\n",
    "con.execute(\"\"\"\n",
    "    COPY df TO 's3://warehouse/test-output/rides.parquet'\n",
    "    (FORMAT 'parquet');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9352ae3e-8e22-4c96-b35e-5ec026c67807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "table = catalog.load_table(\"rideshare.rides\")\n",
    "print(table.snapshots())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b46501a-c615-4c5d-9cb9-a3af1807c430",
   "metadata": {},
   "outputs": [
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name s3://warehouse/rideshare/rides does not exist!\nDid you mean \"pg_sequence\"?\n\nLINE 1: SELECT * FROM 's3://warehouse/rideshare/rides'\n                      ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatalogException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://warehouse/rideshare/rides\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.fetchdf()\n",
      "\u001b[31mCatalogException\u001b[39m: Catalog Error: Table with name s3://warehouse/rideshare/rides does not exist!\nDid you mean \"pg_sequence\"?\n\nLINE 1: SELECT * FROM 's3://warehouse/rideshare/rides'\n                      ^"
     ]
    }
   ],
   "source": [
    "con.execute(\"SELECT * FROM 's3://warehouse/rideshare/rides'\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f02af3b-b1cb-49a2-915b-be7083830d49",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Unhandled options found: catalog_type, storage_namespace, uri",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      2\u001b[39m \u001b[33;43m    ATTACH \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://warehouse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AS warehouse\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[33;43m    (TYPE ICEBERG,\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[33;43m     CATALOG_TYPE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43m     URI \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp://localhost:8181\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m     STORAGE_NAMESPACE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://warehouse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m     AUTHORIZATION_TYPE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m);\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mError\u001b[39m: Unhandled options found: catalog_type, storage_namespace, uri"
     ]
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "    ATTACH 's3://warehouse' AS warehouse\n",
    "    (TYPE ICEBERG,\n",
    "     CATALOG_TYPE 'rest',\n",
    "     URI 'http://localhost:8181',\n",
    "     STORAGE_NAMESPACE 's3://warehouse',\n",
    "     AUTHORIZATION_TYPE 'none');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ded11cb3-5349-4580-be46-70da2b33a8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7fddf3d58630>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.register(\"rides_df\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa47f81d-ac77-4aff-9a80-87b29919dd6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name s3://warehouse/rideshare/rides does not exist!\nDid you mean \"pg_sequence\"?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatalogException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      2\u001b[39m \u001b[33;43m    INSERT INTO \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://warehouse/rideshare/rides\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[33;43m    SELECT * FROM rides_df\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mCatalogException\u001b[39m: Catalog Error: Table with name s3://warehouse/rideshare/rides does not exist!\nDid you mean \"pg_sequence\"?"
     ]
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "    INSERT INTO 's3://warehouse/rideshare/rides'\n",
    "    SELECT * FROM rides_df\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9d9c063-65e9-4a93-9fa2-204f4d938210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rides(\n",
       "  1: ride_id: required uuid,\n",
       "  2: driver_id: optional string,\n",
       "  3: customer_id: optional string,\n",
       "  4: pickup_time: optional timestamp,\n",
       "  5: dropoff_time: optional timestamp,\n",
       "  6: fare: optional double,\n",
       "  7: pickup_location: optional string,\n",
       "  8: dropoff_location: optional string\n",
       "),\n",
       "partition by: [pickup_time],\n",
       "sort order: [],\n",
       "snapshot: null"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define schema using NestedField\n",
    "\n",
    "# ❗ Iceberg requires all fields to have stable, explicit IDs.\n",
    "# This is critical for schema evolution and tracking changes over time.\n",
    "# That's why we use NestedField() — each field has:\n",
    "# - field_id: required, stable numeric ID\n",
    "# - name: field name\n",
    "# - field_type: Iceberg data type\n",
    "# - required: whether the field is NOT NULL\n",
    "\n",
    "rides_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"ride_id\", field_type=UUIDType(), required=True),\n",
    "    NestedField(field_id=2, name=\"driver_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"customer_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=4, name=\"pickup_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=5, name=\"dropoff_time\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=6, name=\"fare\", field_type=DoubleType(), required=False),\n",
    "    NestedField(field_id=7, name=\"pickup_location\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=8, name=\"dropoff_location\", field_type=StringType(), required=False)\n",
    ")\n",
    "\n",
    "from pyiceberg.partitioning import PartitionSpec\n",
    "\n",
    "rides_partition_spec = PartitionSpec(\n",
    "    fields=[\n",
    "        PartitionField(\n",
    "            source_id=4,\n",
    "            field_id=1000,\n",
    "            transform=\"identity\",\n",
    "            name=\"pickup_time\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop if exists (optional)\n",
    "# try:\n",
    "#     catalog.drop_table(identifier=f\"{namespace}.rides\")\n",
    "# except NoSuchTableError:\n",
    "#     pass\n",
    "\n",
    "catalog.create_table(\n",
    "    identifier=f\"{namespace}.rides\",\n",
    "    schema=rides_schema,\n",
    "    partition_spec=rides_partition_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fe75702-a73d-442e-8a83-91b2d1cf0525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drivers(\n",
       "  1: driver_id: required string,\n",
       "  2: full_name: optional string,\n",
       "  3: city: optional string,\n",
       "  4: active: optional boolean,\n",
       "  5: rating: optional int,\n",
       "  6: last_updated: optional timestamp\n",
       "),\n",
       "partition by: [city],\n",
       "sort order: [],\n",
       "snapshot: null"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drivers_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"driver_id\", field_type=StringType(), required=True),\n",
    "    NestedField(field_id=2, name=\"full_name\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"city\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=4, name=\"active\", field_type=BooleanType(), required=False),\n",
    "    NestedField(field_id=5, name=\"rating\", field_type=IntegerType(), required=False),\n",
    "    NestedField(field_id=6, name=\"last_updated\", field_type=TimestampType(), required=False)\n",
    ")\n",
    "\n",
    "# Partition by city (field_id=3)\n",
    "drivers_partition_spec = PartitionSpec(\n",
    "    fields=[\n",
    "        PartitionField(\n",
    "            source_id=3,\n",
    "            field_id=1001,\n",
    "            transform=\"identity\",\n",
    "            name=\"city\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the table\n",
    "catalog.create_table(\n",
    "    identifier=f\"{namespace}.drivers\",\n",
    "    schema=drivers_schema,\n",
    "    partition_spec=drivers_partition_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57587b82-7127-4cde-a687-7565dc432278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payments(\n",
       "  1: payment_id: required uuid,\n",
       "  2: ride_id: optional string,\n",
       "  3: customer_id: optional string,\n",
       "  4: amount: optional decimal(10, 2),\n",
       "  5: status: optional string,\n",
       "  6: timestamp: optional timestamp\n",
       "),\n",
       "partition by: [timestamp],\n",
       "sort order: [],\n",
       "snapshot: null"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payments_schema = Schema(\n",
    "    NestedField(field_id=1, name=\"payment_id\", field_type=UUIDType(), required=True),\n",
    "    NestedField(field_id=2, name=\"ride_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=3, name=\"customer_id\", field_type=StringType(), required=False),\n",
    "    NestedField(field_id=4, name=\"amount\", field_type=DecimalType(precision=10, scale=2), required=False),\n",
    "    NestedField(field_id=5, name=\"status\", field_type=StringType(), required=False),  # e.g. \"paid\", \"refunded\"\n",
    "    NestedField(field_id=6, name=\"timestamp\", field_type=TimestampType(), required=False)\n",
    ")\n",
    "\n",
    "# Partition by 'timestamp' (field_id=6)\n",
    "payments_partition_spec = PartitionSpec(\n",
    "    fields=[\n",
    "        PartitionField(\n",
    "            source_id=6,\n",
    "            field_id=1002,\n",
    "            transform=\"identity\",\n",
    "            name=\"timestamp\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "catalog.create_table(\n",
    "    identifier=f\"{namespace}.payments\",\n",
    "    schema=payments_schema,\n",
    "    partition_spec=payments_partition_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecf260-0aad-4c62-83cd-285604e791f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fec48-6179-48db-9323-061c7fe137cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyiceberg.catalog import load_rest\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError, TableAlreadyExistsError\n",
    "import boto3\n",
    "\n",
    "aws_access_key_id = os.environ[\"AWS_ACCESS_KEY_ID\"] # admin\n",
    "aws_secret_access_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"] # password\n",
    "\n",
    "catalog = load_rest(\n",
    "    name=\"rest\",\n",
    "    conf = {\n",
    "        \"uri\": \"http://rest:8181/\",\n",
    "        \"s3.endpoint\": \"http://minio:9000\",\n",
    "        \"s3.access-key\": aws_access_key_id,\n",
    "        \"s3.secret-key\": aws_secret_access_key\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a S3 \"mocked\" client with iceberg user credentials\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio:9000\",  # ✅ Use the container name\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "namespace = \"poc_new\"\n",
    "try:\n",
    "    catalog.create_namespace(namespace)\n",
    "except NamespaceAlreadyExistsError as e:\n",
    "    pass\n",
    "\n",
    "namespaces = catalog.list_namespaces()\n",
    "print(\"Namespaces:\", namespaces)\n",
    "\n",
    "def list_blobs(bucket=None):\n",
    "    \"\"\"\n",
    "    Lists blobs (objects) in a specific S3 bucket or in all buckets.\n",
    "\n",
    "    Parameters:\n",
    "        bucket (str, optional): Bucket name. If not provided, lists objects in all buckets.\n",
    "    \"\"\"\n",
    "    if bucket:\n",
    "        print(f\"\\nObjects in bucket: {bucket}\")\n",
    "        _print_bucket_objects(bucket)\n",
    "    else:\n",
    "        buckets = s3.list_buckets()[\"Buckets\"]\n",
    "        for b in buckets:\n",
    "            bucket_name = b[\"Name\"]\n",
    "            print(f\"\\nObjects in bucket: {bucket_name}\")\n",
    "            _print_bucket_objects(bucket_name)\n",
    "\n",
    "\n",
    "def _print_bucket_objects(bucket_name):\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "    if \"Contents\" in response:\n",
    "        for obj in response[\"Contents\"]:\n",
    "            print(f\" - {obj['Key']}\")\n",
    "    else:\n",
    "        print(\" (Empty)\")\n",
    "\n",
    "list_blobs()\n",
    "\n",
    "df = pa.Table.from_pylist(\n",
    "    [\n",
    "        {\"lat\": 52.371807, \"long\": 4.896029},\n",
    "        {\"lat\": 52.387386, \"long\": 4.646219},\n",
    "        {\"lat\": 52.078663, \"long\": 4.288788},\n",
    "    ],\n",
    ")\n",
    "schema = df.schema\n",
    "\n",
    "table_name = \"coordinates\"\n",
    "table_identifier = f\"{namespace}.{table_name}\"\n",
    "\n",
    "try:\n",
    "    table = catalog.create_table(\n",
    "        identifier=table_identifier,\n",
    "        schema=schema,\n",
    "    )\n",
    "except TableAlreadyExistsError as e:\n",
    "    pass\n",
    "\n",
    "table = catalog.load_table(table_identifier)\n",
    "table.append(df)\n",
    "\n",
    "result = table.scan().to_arrow()\n",
    "print(result)\n",
    "\n",
    "list_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ca8493-19d7-4b62-a72b-2dcc24328b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a S3 \"mocked\" client with iceberg user credentials\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio:9000\",  # ✅ Use the container name\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a20113-91e7-4571-b9bb-2ce3ac88b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = \"poc_new\"\n",
    "try:\n",
    "    catalog.create_namespace(namespace)\n",
    "except NamespaceAlreadyExistsError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d66b663-0f94-4e8b-8f06-731a55f26e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces: [('poc_new',)]\n"
     ]
    }
   ],
   "source": [
    "namespaces = catalog.list_namespaces()\n",
    "print(\"Namespaces:\", namespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d4ec01-436f-4f5c-8643-42f91868007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket=None):\n",
    "    \"\"\"\n",
    "    Lists blobs (objects) in a specific S3 bucket or in all buckets.\n",
    "\n",
    "    Parameters:\n",
    "        bucket (str, optional): Bucket name. If not provided, lists objects in all buckets.\n",
    "    \"\"\"\n",
    "    if bucket:\n",
    "        print(f\"\\nObjects in bucket: {bucket}\")\n",
    "        _print_bucket_objects(bucket)\n",
    "    else:\n",
    "        buckets = s3.list_buckets()[\"Buckets\"]\n",
    "        for b in buckets:\n",
    "            bucket_name = b[\"Name\"]\n",
    "            print(f\"\\nObjects in bucket: {bucket_name}\")\n",
    "            _print_bucket_objects(bucket_name)\n",
    "\n",
    "\n",
    "def _print_bucket_objects(bucket_name):\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "    if \"Contents\" in response:\n",
    "        for obj in response[\"Contents\"]:\n",
    "            print(f\" - {obj['Key']}\")\n",
    "    else:\n",
    "        print(\" (Empty)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc6f896-35b5-4c9f-a1ab-9d551847aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects in bucket: warehouse\n",
      " (Empty)\n"
     ]
    }
   ],
   "source": [
    "list_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb85b42-e823-486a-b480-03e04c1d0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pa.Table.from_pylist(\n",
    "    [\n",
    "        {\"lat\": 52.371807, \"long\": 4.896029},\n",
    "        {\"lat\": 52.387386, \"long\": 4.646219},\n",
    "        {\"lat\": 52.078663, \"long\": 4.288788},\n",
    "    ],\n",
    ")\n",
    "schema = df.schema\n",
    "\n",
    "table_name = \"coordinates\"\n",
    "table_identifier = f\"{namespace}.{table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af0715f-791f-4c14-b3e2-ff8f5b567415",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    table = catalog.create_table(\n",
    "        identifier=table_identifier,\n",
    "        schema=schema,\n",
    "    )\n",
    "except TableAlreadyExistsError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a20b52-0674-4595-8859-5c0db50f14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = catalog.load_table(table_identifier)\n",
    "table.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f11903d-c5be-486c-a1ef-bde9fb571e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "lat: double\n",
      "long: double\n",
      "----\n",
      "lat: [[52.371807,52.387386,52.078663]]\n",
      "long: [[4.896029,4.646219,4.288788]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyiceberg/avro/decoder.py:185: UserWarning: Falling back to pure Python Avro decoder, missing Cython implementation\n",
      "  warnings.warn(\"Falling back to pure Python Avro decoder, missing Cython implementation\")\n"
     ]
    }
   ],
   "source": [
    "result = table.scan().to_arrow()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089ef9ed-32df-4a60-b88b-bf0ec737981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects in bucket: warehouse\n",
      " - poc_new/coordinates/data/00000-0-977d2bf6-fc86-443a-bd63-5e7b06caffbd.parquet\n",
      " - poc_new/coordinates/metadata/00000-ab97f938-d7c6-4d14-8142-eb88f3da9569.metadata.json\n",
      " - poc_new/coordinates/metadata/00001-565668f6-fe20-4ee2-98f6-0fc10bba87c7.metadata.json\n",
      " - poc_new/coordinates/metadata/977d2bf6-fc86-443a-bd63-5e7b06caffbd-m0.avro\n",
      " - poc_new/coordinates/metadata/snap-1981489265837032690-0-977d2bf6-fc86-443a-bd63-5e7b06caffbd.avro\n"
     ]
    }
   ],
   "source": [
    "list_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f593d7-5d7f-4852-a2ac-e9034b880dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
